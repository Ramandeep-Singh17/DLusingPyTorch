{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramandeep-Singh17/DLusingPyTorch/blob/main/2_AutogradPyTorch_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¥ Autograd in PyTorch (Automatic Differentiation)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ What is Autograd?\n",
        "\n",
        "Autograd is a PyTorch feature that automatically calculates derivatives (gradients) of tensors.\n",
        "\n",
        "In simple words:\n",
        "> Autograd = Automatic derivative calculator\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Why Do We Need Autograd?\n",
        "\n",
        "Deep Learning = Optimization problem  \n",
        "Optimization needs â†’ Gradients  \n",
        "Gradients = Derivatives  \n",
        "\n",
        "Neural network training uses:\n",
        "- Backpropagation\n",
        "- Gradient Descent\n",
        "\n",
        "Without derivatives â†’ No weight update â†’ No learning\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Basic Example\n",
        "\n",
        "Let:\n",
        "\n",
        "y = xÂ²  \n",
        "\n",
        "Derivative:\n",
        "\n",
        "dy/dx = 2x  \n",
        "\n",
        "If:\n",
        "x = 2 â†’ dy/dx = 4  \n",
        "x = 3 â†’ dy/dx = 6  \n",
        "\n",
        "We can easily compute slope by changing x.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Now Increase Complexity\n",
        "\n",
        "Suppose:\n",
        "\n",
        "y = xÂ²  \n",
        "z = sin(y)  \n",
        "\n",
        "Now we need:\n",
        "\n",
        "dz/dx  \n",
        "\n",
        "We cannot directly differentiate.  \n",
        "We must use **Chain Rule**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Chain Rule\n",
        "\n",
        "If:\n",
        "\n",
        "z = sin(y)  \n",
        "y = xÂ²  \n",
        "\n",
        "Then:\n",
        "\n",
        "dz/dx = (dz/dy) Ã— (dy/dx)\n",
        "\n",
        "We know:\n",
        "\n",
        "dz/dy = cos(y)  \n",
        "dy/dx = 2x  \n",
        "\n",
        "So:\n",
        "\n",
        "dz/dx = cos(y) Ã— 2x  \n",
        "\n",
        "Since y = xÂ²:\n",
        "\n",
        "dz/dx = 2x cos(xÂ²)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Why This Matters in Deep Learning\n",
        "\n",
        "Neural networks are full of nested functions:\n",
        "\n",
        "Example:\n",
        "\n",
        "u = e^z  \n",
        "z = sin(y)  \n",
        "y = xÂ²  \n",
        "\n",
        "Now derivative:\n",
        "\n",
        "du/dx = (du/dz) Ã— (dz/dy) Ã— (dy/dx)\n",
        "\n",
        "As functions become more nested:\n",
        "- Manual derivative becomes difficult\n",
        "- Human error chances increase\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Where Autograd is Used?\n",
        "\n",
        "- Backpropagation\n",
        "- Loss gradient calculation\n",
        "- Weight updates\n",
        "- Training deep neural networks\n",
        "\n",
        "Every layer depends on previous layer â†’ Chain rule everywhere\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ How Autograd Helps?\n",
        "\n",
        "PyTorch automatically:\n",
        "\n",
        "1. Tracks operations\n",
        "2. Builds computation graph\n",
        "3. Applies chain rule\n",
        "4. Calculates gradients\n",
        "5. Stores gradients in `.grad`\n",
        "\n",
        "No need to manually calculate derivatives.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”¥ Final Understanding\n",
        "\n",
        "- Neural networks = Nested mathematical functions\n",
        "- Training = Derivative calculation\n",
        "- Derivatives use Chain Rule\n",
        "- Manual differentiation is complex\n",
        "- Autograd does it automatically\n",
        "\n",
        "Autograd is the heart of Deep Learning training.\n"
      ],
      "metadata": {
        "id": "6CCzxnGjx2N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dy_dx(x):\n",
        "  return 2*x"
      ],
      "metadata": {
        "id": "bPKGnmCNx6wP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f256ernlf_ce",
        "outputId": "204cac7a-730d-49a7-b712-c8bc7fd9a9d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def dz_dx(x):\n",
        "   return 2*x*math.cos(x**2)"
      ],
      "metadata": {
        "id": "Jl-IgXHFw8G6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dz_dx(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKZKLztZxGi9",
        "outputId": "a34855f8-7fae-4fcd-e0ee-6951c21f91c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.6145744834544478"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Same x2 ka calculation using autograd**"
      ],
      "metadata": {
        "id": "oCefVtlpx1Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "bm_ra2CG21GV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "# tensor me ek value ahi wo scaler\n",
        "#  requires_grad=True (hmne ise true set kiya hai ye bydeafult false hota hai)\n",
        "# agar hm kabhi bhi derivates cahiye yab  requires_grad=True karna padta hai\n"
      ],
      "metadata": {
        "id": "nvMNlqM521TC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x**2"
      ],
      "metadata": {
        "id": "6_iSjcNw21cK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUzlzqUu21qk",
        "outputId": "7e71dd7e-8eb4-4161-c89d-6c1a664f8e36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOVdjAvk4FsY",
        "outputId": "054be93a-0652-4b26-9406-6ab7058cab20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "#  ye hme backward jate waqt dy/dx dega jisse deifferrnion milta hai"
      ],
      "metadata": {
        "id": "hsmrBTVv4r1A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz4fg9YR4s3V",
        "outputId": "33e728b0-7118-4699-c3cd-000f5b3a6203"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¥ Autograd in PyTorch (Core Idea)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Neural Network = Nested Function\n",
        "\n",
        "Example:\n",
        "\n",
        "y = xÂ²  \n",
        "z = sin(y)  \n",
        "u = e^z  \n",
        "\n",
        "Neural Network bhi exactly aise hi hota hai:\n",
        "\n",
        "Output = f3(f2(f1(x)))\n",
        "\n",
        "ðŸ‘‰ Har layer ek function hai  \n",
        "ðŸ‘‰ Deep network = Highly nested function  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Forward Computation\n",
        "\n",
        "Forward pass me:\n",
        "\n",
        "1. Linear transformation â†’ z = wx + b  \n",
        "2. Activation â†’ sigmoid(z)  \n",
        "3. Loss calculation  \n",
        "\n",
        "Mathematically:\n",
        "\n",
        "Loss = L( sigmoid(wx + b) )\n",
        "\n",
        "Ye pura ek **nested function** hai.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Problem: Derivatives\n",
        "\n",
        "Training ke liye hume chahiye:\n",
        "\n",
        "dL/dw  \n",
        "dL/db  \n",
        "\n",
        "Agar network deep ho gaya:\n",
        "\n",
        "Layer1 â†’ Layer2 â†’ Layer3 â†’ ... â†’ Output  \n",
        "\n",
        "To derivative nikalna hoga:\n",
        "\n",
        "dL/dw = (chain rule applied many times)\n",
        "\n",
        "Manual differentiation:\n",
        "- Bahut lengthy\n",
        "- Error-prone\n",
        "- Practically impossible for deep networks\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Solution â†’ AUTOGRAD\n",
        "\n",
        "Autograd automatically:\n",
        "\n",
        "- Tracks operations\n",
        "- Builds computation graph\n",
        "- Applies chain rule\n",
        "- Calculates gradients\n",
        "\n",
        "No manual derivative required.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”¹ requires_grad = True\n",
        "\n",
        "When we write:\n",
        "\n",
        "```\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "```\n",
        "\n",
        "It tells PyTorch:\n",
        "\n",
        "ðŸ‘‰ \"Is tensor ka gradient calculate karna hai\"\n",
        "\n",
        "Then PyTorch:\n",
        "\n",
        "1. Tracks all operations on x  \n",
        "2. Creates a computation graph  \n",
        "3. During `.backward()`  \n",
        "4. Calculates dy/dx automatically  \n",
        "\n",
        "Gradient stored in:\n",
        "\n",
        "```\n",
        "x.grad\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ How It Works (Simple Flow)\n",
        "\n",
        "Forward pass:\n",
        "x â†’ square â†’ sin â†’ loss  \n",
        "\n",
        "Backward pass:\n",
        "loss.backward()\n",
        "\n",
        "PyTorch automatically computes:\n",
        "\n",
        "dloss/dx  \n",
        "\n",
        "Using chain rule internally.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”¥ THE WHY (Very Important â€“ Hinglish)\n",
        "\n",
        "Neural network ek nested function hota hai.  \n",
        "Jaise jaise network deep hota hai, derivative manually nikalna practically impossible ho jata hai.  \n",
        "\n",
        "Isliye hum Autograd use karte hain jo automatically chain rule apply karke gradients calculate karta hai.\n",
        "\n",
        "Without Autograd â†’ No Backpropagation â†’ No Training.\n"
      ],
      "metadata": {
        "id": "aFRvzv8M2p96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def dz_dx(x):\n",
        "    return 2 * x * math.cos(x**2)"
      ],
      "metadata": {
        "id": "d_66YDf8gFwS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dz_dx(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxPYLAIwgLSW",
        "outputId": "6bed319e-f4ac-45f5-95e3-3dae984bd051"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-7.661275842587077"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abhi jo cheez hm manually calaculate kar rhe hai z=sin(y), dz/dx nikal rhe hai use ab pytorch se nikal lenhe AutoGrad se**"
      ],
      "metadata": {
        "id": "KheSNYlr263V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(4.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "tYHUyJhBgO9d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "#x->sq--y-> z\n",
        "# jab hm forward ja rhe hai tab hme y=x2 mil rha hai so y =16 aa rha hai"
      ],
      "metadata": {
        "id": "17J3r8mZ8ebE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.sin(y)"
      ],
      "metadata": {
        "id": "5YDYJEwJ8f7u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_RxpqkS-EpK",
        "outputId": "1bdf4a9d-74d6-4e1f-fa5b-e9aa23aec12e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAfr7jCr-Fl8",
        "outputId": "969f605f-bef2-427c-d50a-d461368b3304"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(16., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mspFqkAS-F3h",
        "outputId": "e1b6efce-d9e9-493a-8d23-d9b35acfaccc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.2879, grad_fn=<SinBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ye sab mathemaitical intuion hai ki internally jayega and phir forward se kuch calculate hoga and phir kuch backward se but ye point bas itna hai ki jisb+ka bhi nikalna ho derivatives  *usme backward laga dena.***"
      ],
      "metadata": {
        "id": "av6-XxT54i8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "L_gisRUH-GNJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad\n",
        "#   ye hm likte hai uski valuse dekhe ke liye jiska ifferention dekhna chahte hai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6avVSli-nzp",
        "outputId": "be699287-e89e-463a-a2c9-bb72e73322f0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-7.6613)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.grad #The .grad attribute of a Tensor that is not a leaf Tensor is being accessed."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGO166vr-pvc",
        "outputId": "a9546075-f594-4576-9080-12159ea656da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-960029856.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
            "  y.grad #The .grad attribute of a Tensor that is not a leaf Tensor is being accessed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Inputs\n",
        "x = torch.tensor(6.7)  # Input feature\n",
        "y = torch.tensor(0.0)  # True label (binary)\n",
        "\n",
        "w = torch.tensor(1.0)  # Weight\n",
        "b = torch.tensor(0.0)  # Bias"
      ],
      "metadata": {
        "id": "IDA7vJD6TERe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”¹ Binary Cross Entropy (BCE) Loss\n",
        "\n",
        "Binary classification ke case me loss function:\n",
        "\n",
        "L = - [ y_target * log(y_pred) + (1 - y_target) * log(1 - y_pred) ]\n",
        "\n",
        "Where:\n",
        "\n",
        "- y_target â†’ Actual label (0 or 1)\n",
        "- y_pred â†’ Model prediction (after sigmoid, between 0 and 1)\n",
        "\n",
        "ðŸ‘‰ If prediction correct hoga â†’ loss small  \n",
        "ðŸ‘‰ If prediction wrong hoga â†’ loss large  \n",
        "\n",
        "Used when:\n",
        "- Output layer me **Sigmoid**\n",
        "- Binary classification problem (0/1)\n"
      ],
      "metadata": {
        "id": "9TmHEAzq8j4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Cross-Entropy Loss for scalar\n",
        "def binary_cross_entropy_loss(prediction, target):\n",
        "    epsilon = 1e-8  # To prevent log(0)\n",
        "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
        "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
      ],
      "metadata": {
        "id": "CNcSnxKFVw6_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "z = w * x + b  # Weighted sum (linear part)\n",
        "y_pred = torch.sigmoid(z)  # Predicted probability i.e ^y\n",
        "\n",
        "# Compute binary cross-entropy loss\n",
        "loss = binary_cross_entropy_loss(y_pred, y)"
      ],
      "metadata": {
        "id": "Ysa6OOlAVzkI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0U1dFI2aX4n",
        "outputId": "6d77a7dc-c1b9-4e9b-cd52-ea5081c5cb0a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derivatives:\n",
        "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
        "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
        "\n",
        "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred * (1 - y_pred)\n",
        "\n",
        "# 3. dz/dw and dz/db: z with respect to w and b\n",
        "dz_dw = x  # dz/dw = x\n",
        "dz_db = 1  # dz/db = 1 (bias contributes directly to z)\n",
        "\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
      ],
      "metadata": {
        "id": "N31_2LUfV2cb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
        "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSP5rszqV5GG",
        "outputId": "e892fb13-e8c8-438b-94b7-d49aea23dae0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
            "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using autgrad to do the same thing easily**"
      ],
      "metadata": {
        "id": "9WWb8fQN8-5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)"
      ],
      "metadata": {
        "id": "3xdGCYz8V-Rh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "9NTVaauoa1CZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTm9Pmf3a8cD",
        "outputId": "6334c615-1da1-4876-ba31-0d54e24a5347"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbWal3sIa858",
        "outputId": "54b65ba0-17f6-4b5f-82b2-d14881df3641"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”¹ What is it?\n",
        "\n",
        "z = w*x + b  \n",
        "\n",
        "Where:\n",
        "\n",
        "- x â†’ input\n",
        "- w â†’ weight\n",
        "- b â†’ bias\n",
        "- z â†’ output before activation\n"
      ],
      "metadata": {
        "id": "wO_m5f7o9bxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = w*x + b\n",
        "z# mathemaitical formula predefined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvuriIW3a9OY",
        "outputId": "80919b11-42b8-498a-da05-16e39e82e506"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.sigmoid(z)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUEAvbpcbBlU",
        "outputId": "317f488f-c98c-4e35-e953-19e18224eb7c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKaUqr18bIBd",
        "outputId": "349ae81d-92b2-4dee-8ec3-c42231e23dfc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward() # backward move kar rhe hai taki uska differention nikal jayee"
      ],
      "metadata": {
        "id": "Q891gtHabNoB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O27utwfFbiw1",
        "outputId": "8640e868-3836-4138-9054-e1b606dc04a7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6918)\n",
            "tensor(0.9988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)# ab vecor input de rhe hai"
      ],
      "metadata": {
        "id": "yuKGIgDThWq1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61pd4iJuijxK",
        "outputId": "065ea3ae-6246-4c48-e584-7eea33ce6232"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = (x**2).mean()\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFDXqOw7ikIM",
        "outputId": "b5c63a14-ae64-4f28-f025-fcbc1c8d0d9f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.6667, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "tWE_JM2xio9Q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad# ab teen gradient mang rhe hai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le70Drbkit_2",
        "outputId": "0d8e8f97-a6a7-4823-d67c-8eb9777571bb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¥ Clearing Gradients in PyTorch\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Problem: Gradient Accumulation\n",
        "\n",
        "In PyTorch:\n",
        "\n",
        "When we call:\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "Gradients are **added (accumulated)** to existing gradients.\n",
        "\n",
        "They are NOT automatically replaced.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ What Happens If We Don't Clear Gradients?\n",
        "\n",
        "If we run backward multiple times:\n",
        "\n",
        "1st backward â†’ grad = g1  \n",
        "2nd backward â†’ grad = g1 + g2  \n",
        "3rd backward â†’ grad = g1 + g2 + g3  \n",
        "\n",
        "Gradients keep accumulating.\n",
        "\n",
        "This leads to:\n",
        "- Wrong weight updates\n",
        "- Incorrect training\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Why Does PyTorch Accumulate Gradients?\n",
        "\n",
        "Because sometimes we intentionally:\n",
        "\n",
        "- Accumulate gradients over multiple batches\n",
        "- Use small batch sizes\n",
        "- Simulate large batch training\n",
        "\n",
        "So PyTorch does not reset gradients automatically.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Solution: Clear Gradients Before Backward\n",
        "\n",
        "Before every backward pass:\n",
        "\n",
        "```\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "This sets gradients to zero.\n",
        "\n",
        "Correct training flow:\n",
        "\n",
        "1. optimizer.zero_grad()\n",
        "2. forward pass\n",
        "3. loss calculation\n",
        "4. loss.backward()\n",
        "5. optimizer.step()\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸ”¥ Important Understanding (Hinglish)\n",
        "\n",
        "Agar hum multiple baar backward chalate hain aur gradients clear nahi karte,  \n",
        "to naye gradients purane gradients me add hote rehte hain.  \n",
        "\n",
        "Isliye har training step se pehle  \n",
        "gradients ko zero karna zaroori hai.\n"
      ],
      "metadata": {
        "id": "lrPXCIAaBqvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clearing grad\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcUAtKSFivZ1",
        "outputId": "ee0c27bb-e5b2-49ca-aa80-5dbf7124f1ab"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGml8iABkLUf",
        "outputId": "e071a1c1-2d16-468a-9f27-61f264a6fd14"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "1EkADF0enBdi"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60O_yfrInD_T",
        "outputId": "ea225429-5997-421d-f3d9-3bb87291551c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxQMMaFqoB4u",
        "outputId": "8a9be42c-ef53-41df-fd83-8d07e0aa6146"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¥ Disable Gradient Tracking\n",
        "\n",
        "## ðŸ”¹ Concept\n",
        "\n",
        "Training ke time:\n",
        "- Gradient chahiye\n",
        "- Backward pass ON\n",
        "\n",
        "Prediction ke time:\n",
        "- Gradient nahi chahiye\n",
        "- Backward pass OFF\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Kaise band kare?\n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "    output = model(x)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Kyun band kare?\n",
        "\n",
        "- Memory bachegi\n",
        "- Model fast chalega\n",
        "- Computation graph nahi banega\n",
        "\n",
        "---\n",
        "\n",
        "Simple baat:\n",
        "Training me gradient ON  \n",
        "Prediction me gradient OFF\n"
      ],
      "metadata": {
        "id": "Wafj3OLlCQeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# disable gradient tracking\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGyP-K6ooelo",
        "outputId": "e1412855-e55e-4aa6-931d-927f47b9d199"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL3tQ2LAq0n-",
        "outputId": "a96cd83b-9a5b-4b6b-89de-29546f3b3146"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "yyOCApZPr7zm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQYxP_xr_1l",
        "outputId": "9cb93e8f-4b2e-4f59-dcae-a7f01a1092dd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# option 1 - requires_grad_(False)\n",
        "# option 2 - detach()\n",
        "# option 3 - torch.no_grad()"
      ],
      "metadata": {
        "id": "nSuNmGoxq2ME"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-3J1W7BsLiK",
        "outputId": "95753099-4549-45e0-9207-cde8ec56258b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eqdqlwzsPR_",
        "outputId": "eb6bc977-2496-4d46-9287-7f7738a037ab"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2"
      ],
      "metadata": {
        "id": "I6hTHHQZsQbP"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWh8lxdEsdRa",
        "outputId": "06346ad3-2a66-4a29-d5a0-cf096f8ea499"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()# dekho  upper y ka gradient func wala aatribute nhi dikh rha haiso hm y ko call nhi kar sakte hai."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "DrW7KpiBsd6Z",
        "outputId": "ed14101d-95ab-4607-936a-ae67817a2165"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1055444503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Ct0xomsgw7",
        "outputId": "113e39ce-5378-455b-968c-5f621f2116a2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”¹ detach()\n",
        "\n",
        "`detach()` tensor ko computation graph se alag kar deta hai taaki uska gradient track na ho.\n"
      ],
      "metadata": {
        "id": "0Q4GttzSTb7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = x.detach()\n",
        "z\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pslVLzLsmO0",
        "outputId": "954f6af9-d787-429f-a5ba-b86f1e800975"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2"
      ],
      "metadata": {
        "id": "tXkAjBEcsp_C"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCm5rnLwsuMt",
        "outputId": "b240d565-a661-4b13-8bd0-cb96f7364910"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y1 = z ** 2\n",
        "y1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zK-PB97su18",
        "outputId": "8c3ad87f-1a21-4ecb-b48e-325b750beea3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "_lQLYSmesxeX"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "yvhtrhXMszVu",
        "outputId": "d1337d05-3843-41d8-90eb-7977bfaf9c0b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3844781142.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrpCL8cbs0us",
        "outputId": "627d8c17-b208-402d-8d7e-d9202fe23ac7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2"
      ],
      "metadata": {
        "id": "n88-a1bxs569"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMsS-KPGs-x9",
        "outputId": "4843e8fe-4768-4004-938d-19e3b6ffe9d8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "GYN6bHVVs_OA"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oy2Bz9CYtBir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}